# AI_chatBot
허깅페이스 트랜스포머 기반 AI 챗봇 제작(한국메타버스산업협회) 교육 수료 내용 정리

---

#### 자연어 처리(NLP: Natural Language Processing) 과정
##### 1. 대규모 데이터셋
1) 텍스트 데이터: NLP 모델의 학습에서 사용되는 텍스트 데이터는 인터넷에서 수집된 다양한 출처로부터 얻음. 웹페이지, 뉴스 기사, 블로그 게시물, 소셜 미디어 게시물, 과학 논문, 책 등 다양한 형식의 텍스트 포함.
     - (대표적인 텍스트 데이터셋: OpenAI-WebText, Common Crawl 등)
2) 이미지 데이터: 컴퓨터 비전 모델의 학습에 사용되는 이미지 데이터셋은 다양한 출처와 카테고리의 이미지들로 구성. 사물인식, 풍경, 동물, 사람, 건축물 등 다양한 주제의 이미지를 포함.
     - (대표적인 이미지 데이터셋: ImageNet, COCO, OpenImages 등)
3) 음성 데이터; 음성 인식 및 생성 모델의 학습에 사용되는 데이터셋은 다양한 언어, 방언, 환경 및 연령대의 발화자로부터 수집된 음성 녹음을 포함. 음성 인식, 음성 합성 및 언어 이해를 위한 모델 학습에 사용.
     - (대표적인 음성 데이터셋: LibriSpeech, VoxCeleb, Common Voice)

##### 2. 비지도 학습
1) 지도 학습: 입력 데이터와 해당 레이블을 사용하여 모델을 학습시키는 방식.
   - (정확도↑/많은 레이블이 있는 데이터 필요/주로 분류, 회귀 작업에 사용/예측 결과에 대한 명확한 정답이 존재할 때 적용)
2) 비지도 학습: 레이블이 없는 입력 데이터를 사용하여 모델이 스스로 패턴을 학습하고 그룹화하는 방식
   - (레이블 없는 데이터 사용 가능/데이터의 숨겨진 패턴 및 구조 발견/주로 군집화, 차원 축소, 생성 모델 등에 사용/입력 데이터의 내재된 구조와 관계를 이해할 때 적용)
3) 강화 학습: 에이전트가 환경과 상호작용하면서 보상을 최대화하는 방식으로 학습하는 방식
   - (시행착오를 통한 학습/주로 의사결정, 게임, 로봇 공학 등에 사용/보상 함수를 통해 성능을 평가 및 개선/동적 환경에서 의사결정을 내릴 때 적합)

##### 3. 트랜스포머 아키텍처
- Word2Vec(2013): 단어의 의미를 벡터로 표현하는 방법을 제시하여 NLP 분야에 혁신
- Sequence to Sequence(2014): 인코더와 디코더 아키텍처를 사용하여 두 시퀀스 간의 매핑을 학습할 수 있는 모델을 제시. 이 모델은 기계 번역, 챗봇 등 다양한 NLP 응용 분야에 적용.
- Long Short-Term Memory;LSTM(2015): 장기 종속성 문제를 해결하기 위해 고안된 순환 신경망(RNN) 변형으로, 시계열 데이터 및 텍스트 데이터 처리에 유용하게 사용.
- Gated Recurrent Unit;GRU(2015): LSTM의 간소화된 버전으로, 계산 효율성이 높고 LSTM과 유사한 성능 달성.
- Attention Mechanism(2015): 입력 시퀀스의 다양한 부분에 가중치를 부여하는 방법을 제시하여 Seq2Seq 모델의 성능을 크게 향상. 이후 다양한 변형이 등장하며 NLP 분야에 광범위하게 적용.
- Transformer(2017): "Attention is all you need" 논문에서 제안된 트랜스포머는 Attention Mechanism을 활용하여 RNN, CNN 등을 대체. 다양한 분야에서 가장 널리 사용되는 아키텍처.

##### 4. 전이 학습(Transfer Learning)
기반 모델(Foundation Model)은 대규모 데이터셋에서 사전 학습된 AI 모델로써 다양한 자연어 처리(NLP) 작업과 관련된 문제들을 해결할 수 있는 기반이 되며, 전이 학습(Transfer Langauge)을 통해 특정 작업에 맞게 미세 조정(Fine-tuning)되어 사용된다. 전이 학습은 기존에 학습된 모델의 지식을 새로운 관련 작업에 적용하는 방법. 기술적으로는, 기존에 학습된 모델의 가중치와 특성을 사용하여 새로운 작업을 해결하는 데 도움이 되도록 모델을 미세 조정하는 과정.
- 전이학습의 특징:
  1) 데이터 효율성: 전이 학습은 적은 양의 데이터로도 조은 성능을 달성할 수 있음. 이는 이미 사전 학습된 모델이 일반적인 패턴과 구조를 학습했기 때문.
  2) 학습 속도: 사전 학습된 모델의 지식을 활용하기 때문에, 전이 학습은 새로운 작업에 대한 학습 속도를 높여줌.
  3) 범용성; Foundation Model은 다양한 NLP 작업에 적용 가능하며, 전이 학습을 통해 특정 작업에 맞게 미세 조정할 수 있다.
- 전이 학습 예시: GPT-3(기계 번역, 감성 분석, 질문-답변 시스템, 텍스트 요약)

##### 5. 다중 모달(Multi-modal) 처리
- 데이터셋: 여러 모달리티(텍스트, 이미지, 오디오 등)를 포함하는 대규모의 데이터셋 필요
- 훈련 전략: 효과적인 훈련 전략이 필요. 여러 모달리티 간의 상호작용과 통합을 위해 적절한 손실 함수와 최적화 알고리즘이 사용.
- 전이 학습: 각 모달리티에 대한 전이 학습이 가능해야 하며, 새로운 작업에 대해 모델을 미세 조정(Fine Tuning)
- 인터페이스: 사용자와의 인터랙션을 최적화하기 위해, 모델은 다양한 모달리티에 대한 입력과 출력을 처리할 수 있는 인터페이스를 제공.
- 성능 최적화: Multimodal Foundation Models는 처리 속도와 정확도가 높아야 하며, 실시간 응용 프로그램에서 사용할 수 있도록 성능 최적화.

---

  #### 운영환경
  - HCI(Human-Computer Interaction): Windows Terminal, Python notebook(jupyter)
  - GPU 가속: NVidia GPU(Cuda driver)
 
---

  #### 트랜스포머
  : 트랜스포머는 언어 뿐만 아니라 이미지, 사운드, 영상 등을 포함하여 CNN보다 다양한 포맷을 다룸.
    → 하나의 알고리즘으로 여러 기능을 수행할 수 있음(Attention Mechanism)

